# -*- coding: utf-8 -*-
"""train_cyclegan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IHX0Cr8R30sikgOQd3T8Wu7FScw1aS0M
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import os
import itertools
import matplotlib.pyplot as plt
import sys
sys.path.append('/content/ultrasound-needle-eraser')

from src.cyclegan.datasets import CycleGANDataset
from src.cyclegan.model import ResnetGenerator, PatchDiscriminator

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_SIZE = 256
BATCH_SIZE = 4
LR = 2e-4
EPOCHS = 20
LAMBDA_CYCLE = 10.0
LAMBDA_ID = 5.0
CHECKPOINT_DIR = "./checkpoints"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

RAW_US_PATH = "/content/drive/MyDrive/zk6scwv52p-2/Needle segmentation/TestDatabase/Images"
ENH_US_PATH = "/content/drive/MyDrive/EnhancedUltrasound"

SAVE_SAMPLES = "./results/samples"
SAVE_CHECKPOINTS = "./results/checkpoints"

os.makedirs(SAVE_SAMPLES, exist_ok=True)
os.makedirs(SAVE_CHECKPOINTS, exist_ok=True)

from torchvision.utils import save_image

def save_fake_samples(fake_A, fake_B, step):
    save_image(fake_A, f"{SAVE_SAMPLES}/fakeA_{step}.png", normalize=True)
    save_image(fake_B, f"{SAVE_SAMPLES}/fakeB_{step}.png", normalize=True)

def train():
    #DATASET
    dataset = CycleGANDataset(
        domainA_dir=RAW_US_PATH,
        domainB_dir=ENH_US_PATH,
        img_size=IMG_SIZE
    )
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    #MODELS
    G_AB = ResnetGenerator().to(DEVICE)   #raw --> enhanced
    G_BA = ResnetGenerator().to(DEVICE)   #enhanced --> raw

    D_A = PatchDiscriminator().to(DEVICE)
    D_B = PatchDiscriminator().to(DEVICE)

    opt_G = torch.optim.Adam(
        list(G_AB.parameters()) + list(G_BA.parameters()),
        lr=LR, betas=(0.5, 0.999)
    )
    opt_D = torch.optim.Adam(
        list(D_A.parameters()) + list(D_B.parameters()),
        lr=LR, betas=(0.5, 0.999)
    )

    adv_loss = torch.nn.MSELoss()
    cycle_loss = torch.nn.L1Loss()
    id_loss_fn = torch.nn.L1Loss()

    step = 0

    for epoch in range(EPOCHS):
        for batch in loader:
            real_A = batch["A"].to(DEVICE)
            real_B = batch["B"].to(DEVICE)

            with torch.no_grad():
                fake_B = G_AB(real_A)
                fake_A = G_BA(real_B)

            D_A_real = D_A(real_A)
            D_A_fake = D_A(fake_A.detach())
            D_A_loss = adv_loss(D_A_real, torch.ones_like(D_A_real)) + \
                       adv_loss(D_A_fake, torch.zeros_like(D_A_fake))

            D_B_real = D_B(real_B)
            D_B_fake = D_B(fake_B.detach())
            D_B_loss = adv_loss(D_B_real, torch.ones_like(D_B_real)) + \
                       adv_loss(D_B_fake, torch.zeros_like(D_B_fake))

            D_loss = (D_A_loss + D_B_loss) * 0.5

            opt_D.zero_grad()
            D_loss.backward()
            opt_D.step()

            fake_B = G_AB(real_A)
            fake_A = G_BA(real_B)

            G_adv_A = adv_loss(D_A(fake_A), torch.ones_like(D_A(fake_A)))
            G_adv_B = adv_loss(D_B(fake_B), torch.ones_like(D_B(fake_B)))

            rec_A = G_BA(fake_B)
            rec_B = G_AB(fake_A)

            cycle_A = cycle_loss(rec_A, real_A)
            cycle_B = cycle_loss(rec_B, real_B)

            id_A = id_loss_fn(G_BA(real_A), real_A)
            id_B = id_loss_fn(G_AB(real_B), real_B)

            G_loss = G_adv_A + G_adv_B + \
                     LAMBDA_CYCLE * (cycle_A + cycle_B) + \
                     LAMBDA_ID * (id_A + id_B)

            opt_G.zero_grad()
            G_loss.backward()
            opt_G.step()

            if step % 200 == 0:
                print(
                    f"[Epoch {epoch}/{EPOCHS}] Step {step} | "
                    f"D_loss: {D_loss.item():.4f} | G_loss: {G_loss.item():.4f}"
                )
                save_fake_samples(fake_A, fake_B, step)

            step += 1

        torch.save(G_AB.state_dict(), f"{SAVE_CHECKPOINTS}/G_AB_{epoch}.pth")
        torch.save(G_BA.state_dict(), f"{SAVE_CHECKPOINTS}/G_BA_{epoch}.pth")
        torch.save(D_A.state_dict(), f"{SAVE_CHECKPOINTS}/D_A_{epoch}.pth")
        torch.save(D_B.state_dict(), f"{SAVE_CHECKPOINTS}/D_B_{epoch}.pth")

    print("Training complete!")

if __name__ == "__main__":
    train()

